---
title: "Assignment 1 - Language Development in ASD - Power and simulations"
author: "Clement Peters"
date: "24/09/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to the third exciting part of the Language Development in ASD exercise

In this part of the assignment, we try to figure out how a new study should be planned (i.e. how many participants?) in order to have enough power to replicate the findings (ensuring our sample size is adequate, our alpha at 0.05 and our beta at 0.8):
1- if we trust the estimates of the current study. Report the power analysis and comment on what you can (or cannot) use its estimates for.
2- if we are skeptical of the current study. Report the power analysis and comment on what you can (or cannot) use its estimates for.
3- if we only have access to 30 participants. Identify the power for each relevant effect and discuss whether it's worth to run the study and why
The list above is also what you should discuss in your code-less report.


## Learning objectives

- Learn how to calculate statistical power
- Critically appraise how to apply frequentist statistical power

### Exercise 1

How much power does your study have (if your model estimates are quite right)?
- Load your dataset (both training and testing), fit your favorite model, assess power for your effects of interest (probably your interactions).
- Report the power analysis and comment on what you can (or cannot) use its estimates for.
- Test how many participants you would have to have to replicate the findings (assuming the findings are correct)

N.B. Remember that main effects are tricky once you have interactions in the model (same for 2-way interactions w 3-way interactions in the model). If you want to test the power of main effects, run a model excluding the interactions.
N.B. Check this paper: https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504
You will be using:
- powerSim() to calculate power
- powerCurve() to estimate the needed number of participants
- extend() to simulate more participants

```{r}
#load packages
library(pacman)
pacman::p_load(readr, dplyr, stringr, lmerTest, Metrics, caret, merTools, tidyverse, simr)

#Load the data
train_df <- read_csv("cleanedData.csv")
test_df <- read_csv("cleanedDataTest.csv")

#Adding 66 to the test dataframe to match the number of participants of the two
test_df$ID <- test_df$ID + 66

#Merging the two dataframes
alldata <- merge(train_df, test_df, all = T)

#Making a subset for fun
sub_alldata <- alldata %>% 
  dplyr::select(c(CHI_MLU, VISIT, Diagnosis, vIQ1, ID)) %>% 
  na.omit()


#Making a beautiful model
model1 <- lmer(CHI_MLU ~ VISIT*Diagnosis + vIQ1 + (1+VISIT|ID), sub_alldata, REML = F)
summary(model1)

model1_REML <- lmer(CHI_MLU ~ VISIT*Diagnosis + vIQ1 + (1+VISIT|ID), sub_alldata, REML = T)
```

```{r}
##Testing the power of the model estimates, starting with interaction.
#1- if we trust the estimates of the current study. Report the power analysis and comment on what you can (or cannot) use its estimates for.

power_interaction <- powerSim(model1, fixed("VISIT:Diagnosis"), nsim = 100)
power_interaction #100 power

#If I want to make a power calculation of the main effects i can make a model excluding the interaction term, allowing me to assess each main effect.


```
```{r}
#2- if we are skeptical of the current study. Report the power analysis and comment on what you can (or cannot) use its estimates for.

#The power for the interaction term was 100% which is pretty high. We need to estimate if this is because the estimate is pretty high, making the effect "easy" to find. If the estimate was lower, the power of the model (interaction term) would be lower.

#For example
model2 <- model1

#Changing the estimate of the model to a lower value 0.25 --> 0.1
fixef(model2)["VISIT:DiagnosisTD"] <- 0.1

#Running the power analysis again
power_interaction2 <- powerSim(model2, fixed("VISIT:Diagnosis"), nsim = 100)
power_interaction2 #83.00%


```
```{r}
#3- if we only have access to 30 participants. Identify the power for each relevant effect and discuss whether it's worth to run the study and why. The list above is also what you should discuss in your code-less report.

#Looking at the power from our number of participants
curve_interaction_ID <- powerCurve(model1, fixed("VISIT:Diagnosis"), nsim = 10, along = "ID")
plot(curve_interaction_ID)

#It seems we would need only about 20 participants to get the required power of 80%, to find the effect of 0.25. 
```


### Exercise 2

How would you perform a more conservative power analysis?
- Identify and justify a minimum effect size for each of your relevant effects
- take the model from exercise 1 and replace the effects with the minimum effect size that you'd accept.
- assess the power curve by Child.ID, identifying an ideal number of participants to estimate each effect
- if your power estimates do not reach an acceptable threshold simulate additional participants and repeat the previous analysis
- Report the power analysis and comment on what you can (or cannot) use its estimates for.

```{r}
#We have to look online for minimum effect sizes that we'd expect.

#The power for the interaction term was 100% which is pretty high. We need to estimate if this is because the estimate is pretty high, making the effect "easy" to find. If the estimate was lower, the power of the model (interaction term) would be lower.

#For example
model2 <- model1

#Changing the estimate of the model to a lower value 0.25 --> 0.1
fixef(model2)["VISIT:DiagnosisTD"] <- 0.1

#Running the power analysis again
power_interaction2 <- powerSim(model2, fixed("VISIT:Diagnosis"), nsim = 100)
power_interaction2 #83.00%
```


### Exercise 3

Assume you have only the resources to collect 30 kids (15 with ASD and 15 TDs). Identify the power for each relevant effect and discuss whether it's worth to run the study and why

```{r}
#Here we want to run a power curve and see if the power of the estimate is sufficiently high at 30 participants
summary(model1)

power_curve_int_30 <- powerCurve(model1, fixed("VISIT:Diagnosis"), along = "ID", breaks = 1:10, nsim = 20)
plot(power_curve_int_30)

#The same as above but without the breaks, which makes it only look at 10 participants..? 
power_curve_int_nobreak <- powerCurve(model1, fixed("VISIT:Diagnosis"), along = "ID", nsim = 20)
plot(power_curve_int_nobreak)

power_curve_vIQ1 <- powerCurve(model1, fixed("vIQ1"), along = "ID", nsim = 20)


#TESTING THE POWER OF THE RANDOM EFFECT (DON'T KNOW IF IT'S SLOPE OR INT THOUGH)
'power_random_intercept <- powerSim(model1, test = random(), nsim = 100)
power_random_intercept #Maybe the power of the random effect is 0?

power_random_intercept_REML <- powerSim(model1_REML, test = random(), nsim = 100)
power_random_intercept_REML #Still no effect'

#For excercise 3::
#Extracting 15 TD and 15 ASD children
e3data1 <- alldata %>% filter(Diagnosis == "TD", ID < 31)
e3data2 <- alldata %>% filter(Diagnosis == "ASD", ID < 30)
#Merging to one dataframe with 15+15 participants
e3data <- merge(e3data1, e3data2, all = T)


```
